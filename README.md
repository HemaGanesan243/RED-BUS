# RED-BUS
The aim of the Project is to revolutionize the transportation industry by providing a comprehensive solution for collecting, analyzing, and visualizing bus travel data by using Data Scraping done by Selenium, Data Migrating and Storing by SQL and build up the Streamlit Application for user-friendly,efficiency. By following the Procedure the Project was done by Step by Step.


This Project is about Redbus Data Scraping and Filtering with Streamlit Application. First thing we need to Scrap the Data by using Selenium. Followed by below steps are used to do Scraping. At initial setup , the script uses Selenium WebDriver to open a Chrome browser and navigate to specific bus booking pages. And then need to pass the index of the links. before that need to create one list, at the link lists we need to paste the any 10 State Booking Link. Need to create  get_route_links function. This function is designed to extract all the route links from a webpage using Selenium. The function first attempts to find all elements on the page that have the class 'route'. It then extracts the 'href' attribute of each element, which contains the URL of the route. Finally, the function returns a list of all the extracted URLs. This is the way to collect all the route links.  Next step is need to Navigate the Pages for that we need to write code depending on how many pages are available at the particular routes.It will collect all the  routes details by navigating the pages. By using the navigate _and_collect_links(driver) function all the route details will get append at all_links as list . By calling this Function it will Navigate one by one Page. After Navigating to first page need to click view buses button for getting Government Bus Data. For clicking the View Button need to write one function using CSS_SELECTOR CLASS. After Clicking the view bus button we need to Scroll down the Pages till the we get all bus details. For that we need to give some range for send keys Page_Down. all these information need to write within the scroll_down_page function.Next that need to extract all the bus details like Price,Star_rating, bus_type,departure_time,arrival_time,bus_names,seat_availability.For that we need to create one function. after that need to create one empty list bus_data=[]. It will append all the details like route_link,bus_type,star_rating,price,seat_availability,deaparture_time.Before that after creating  extaract_bus_data we need to get URL. After getting URL need to click view buses button and then need to pass Scroll_Down function. it will help to collect all the data about buses by using bus_data = extract_bus_data(driver, all_links) this function it will helps navigate the pages and give the output. need to pass the bus_data at for loop. After this we need to quite our driver.  After that we need to store these data in DataFrame after this need to convert as CSV File.   Again need to do initial setup,   WebDriver to open a Chrome browser. in that driver.get  fuction need to passs the links of index 1. After that same thing will do but for this we can call functions instead of writing the whole code. But Page Navigation will got differ for each State/route.Afte getting all the link of each State need to Convert into DataFrame and thn need to store as CSV file for reading. After getting all the  Ten States of DataFrame we need to Concat those DataFrame.After that need to read as CSV File. By using Regular Expression me extract the route_name from the route_link. After that the Data has all the information and once completed without fail need to convert as CSV File Format.                                                                                        

And then we need to Migrate the Date to SQL. For creating Database at SQL me used myconnection.cursor(). by using some code me created the table and gave the name as bus.After that me used myconnection.cursor() for executing the table name as bus. sql = "insert into redbus.bus values"
for i in range(len(df)):
    myconnection.cursor().execute(f"{sql} {tuple(df.iloc[i])}")
    myconnection.commit()                                                                                                                                                                                                                                                          Using this Code me pushed the Data into the SQL. Like this Migration Part is done.
After that Stremlit Part is Done. For this need to Migrate the data from SQL to Streamlit. After that need to execute mycursor = mydb.cursor(). Need to create  filter_route_name(search_term) fuction for getting Route Name.Using Like Operators the bus_type to filtered at A/C,NON A/C and others. After that need to write some Queries for running at Streamlit. Using time_formate Departure_time  and Arrival_time is done. For Star_raating me used slider option me set min value is 0 and max value is 5. The Customers can choose their Ratings by using sliding option. for same Price me used Slider Option and before that me used some code for Replacing INR values.
After that me kept Submit Button. after clicking the Submit Button we need to write some query inside this submit button SELECT * FROM bus
    WHERE route_name = '{selected_option}' 
    AND LOWER('{bus_type}') LIKE LOWER(CONCAT('%', '{bus_type}', '%'))
    AND star_rating BETWEEN {min_star_rating} AND {max_star_rating} 
    AND CAST(REPLACE(REPLACE(price, 'INR ', ''), ',', '') AS DECIMAL(10, 2)) BETWEEN {price_range[0]} AND {price_range[1]} 
    AND departure_time = '{departure_time.strftime("%H:%M")}'
    AND reaching_time = '{reaching_time.strftime("%H:%M")}' 
After that we need to use mycursor.execute function. and then need to close the database(db).

By using this Solution we can provide real-time bus schedules,Price,Star_rating,Departure_time,Arrival_time,Routes_name,Bus_type, seat availability to customers.It will be eficiency to customers also. 




